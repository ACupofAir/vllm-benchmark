diff --git a/ggml/include/bigdl-opt/ggml-convert.h b/ggml/include/bigdl-opt/ggml-convert.h
index fe5c00ad..b789a317 100644
--- a/ggml/include/bigdl-opt/ggml-convert.h
+++ b/ggml/include/bigdl-opt/ggml-convert.h
@@ -48,6 +48,8 @@ static void convert_ggml_tensor(struct ggml_tensor *cur, void * src){
     ) {
         // create new memory for new layout
         size_t tensor_size = ggml_nbytes(cur);
+        printf("[DEBUG] convert_ggml_tensor tensor name %s, tensor shape %d, %d, %d, %d, tensor size is %d.\n",
+            cur->name, cur->ne[0], cur->ne[1], cur->ne[2], cur->ne[3], tensor_size);
         if (cur->type == GGML_TYPE_Q3_K) {
             tensor_size =  ggml_sycl_nbytes(cur);
         }
diff --git a/ggml/src/ggml-alloc.c b/ggml/src/ggml-alloc.c
index f05fc096..954de27a 100644
--- a/ggml/src/ggml-alloc.c
+++ b/ggml/src/ggml-alloc.c
@@ -1174,6 +1174,8 @@ ggml_backend_buffer_t ggml_backend_alloc_ctx_tensors_from_buft(struct ggml_conte
         }
 
         enum ggml_backend_type tensor_backend_type = t->backend;
+        printf("[DEBUG] current tensor name %s, tensor shape %d, %d, %d, %d, tensor size is %d.\n",
+               t->name, t->ne[0], t->ne[1], t->ne[2], t->ne[3], this_size);
 
         switch(tensor_backend_type)
         {
@@ -1196,6 +1198,8 @@ ggml_backend_buffer_t ggml_backend_alloc_ctx_tensors_from_buft(struct ggml_conte
         };
 
         if (cur_buf_sizes[0] > 0 && (cur_buf_sizes[0] + this_buf_sizes[0]) > max_size) {
+            printf("DEBUG cur_buf_sizes 0 %ld \n", cur_buf_sizes[0]);
+            printf("DEBUG cur_buf_sizes 1 %ld \n", cur_buf_sizes[1]);
             // assume TP mode must alloc tensor on device 1...
             // TPTODO: a little trickly here
             bool is_TP_mode = cur_buf_sizes[1] != 0;
@@ -1228,12 +1232,16 @@ ggml_backend_buffer_t ggml_backend_alloc_ctx_tensors_from_buft(struct ggml_conte
             for(int dev_id = 0; dev_id < device_num; dev_id++){
                 // allreduce buffer
                 cur_buf_sizes[dev_id] += this_buf_sizes[dev_id];
+                printf("DEBUG this buf %d %ld \n", dev_id, this_buf_sizes[dev_id]);
+                printf("DEBUG cur_buf_sizes %d %ld \n", dev_id, cur_buf_sizes[dev_id]);
             }
         }
     }
 
     // allocate remaining tensors
     if (cur_buf_sizes[0] > 0) {
+        printf("DEBUG remain cur_buf_sizes 0 %ld \n", cur_buf_sizes[0]);
+        printf("DEBUG remain cur_buf_sizes 1 %ld \n", cur_buf_sizes[1]);
         // assume TP mode must alloc tensor on device 1...
         // TPTODO: a little trickly here
         bool is_TP_mode = cur_buf_sizes[1] != 0;
diff --git a/ggml/src/ggml-sycl/ggml-sycl.cpp b/ggml/src/ggml-sycl/ggml-sycl.cpp
index 1e05a93a..8bddf2eb 100644
--- a/ggml/src/ggml-sycl/ggml-sycl.cpp
+++ b/ggml/src/ggml-sycl/ggml-sycl.cpp
@@ -1030,6 +1030,8 @@ ggml_backend_sycl_hybrid_buffer_set_tensor(ggml_backend_buffer_t buffer,
                                            size_t offset, size_t size) try {
     // Filter tensors not invoke in TP
     // They will by malloced on default device(usually device 0).
+    printf("[DEBUG] current tensor name %s, tensor shape %d, %d, %d, %d, tensor size is %d.\n",
+        tensor->name, tensor->ne[0], tensor->ne[1], tensor->ne[2], tensor->ne[3], size);
     ggml_backend_type tensor_backend_type = tensor->backend;
     if (tensor_backend_type == GGML_BACKEND_TYPE_CPU || tensor_backend_type == GGML_BACKEND_TYPE_GPU) {
         // all layer not splited in tp fashion should be malloced on device 0 only.
@@ -1063,6 +1065,8 @@ ggml_backend_sycl_hybrid_buffer_set_tensor(ggml_backend_buffer_t buffer,
             get_row_split(&row_low, &row_high, tensor, buft_ctx->tensor_split, i);
 
             int64_t nrows_split = row_high - row_low;
+            printf("DEBUG tensor ne1 %d %d \n", i, tensor->ne[1]);
+            printf("DEBUG nrows_split row_high row_low %d %d %d \n", nrows_split, row_high, row_low);
             if (nrows_split == 0) {
                 continue;
             }
